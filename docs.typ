#set page(numbering: "I")


#align(center+horizon)[
#text(size: 24pt)[Training VAE on ARC-AGI-2 Dataset]
]

#show heading: it => block({
  box(height: 12pt, it)
})

#align(bottom, outline())

#align(bottom + right)[#datetime.today().display()]

#pagebreak()

#set page(numbering: "1")
#counter(page).update(1)

#set par(justify: true)
#set heading(numbering: "1.1")

#show link: set text(fill: blue)

= Overview

In this experiment, we explore Variational Autoencoders (VAE) and Conditional Variational Autoencoders (CVAE). We first implement and test these models on the MNIST dataset to validate their functionality. Then, we adapt and train these models on the ARC-AGI-2 dataset, which presents a more complex challenge.

= Analysis of VAE and CVAE

VAE (Variational Autoencoder) is a generative model that learns to encode input data into a latent space and then decode from that space back to the original data distribution.

VAE has many different variants. One common variant is CVAE (Conditional Variational Autoencoder), which extends VAE by conditioning the generation process on additional information, such as class labels. This allows CVAE to generate data that is specific to a given condition.

The implementation of VAE and CVAE is in the `models/` directory. And you can test them on the MNIST dataset using `mnist.py`.

```bash
# use default parameters
uv run ./mnist.py --model-type vae
uv run ./mnist.py --model-type cvae
# Or custom parameters
uv run ./mnist.py --model-type vae --batch-size 64 --epochs 50 --latent-dim 8 --hidden-dims 256
uv run ./mnist.py --model-type cvae --batch-size 64 --epochs 50 --latent-dim 8 --hidden-dims 256
```

Both VAE and CVAE work on the MNIST dataset, producing good quality generated images. But CVAE's results are significantly better than VAE's, as it leverages the label information during training and generation.

The test results on MNIST with default parameters are shown below. In additional, CVAE allows for controlled generation by specifying the class label during sampling. @cvae_test is an example of generating digits 0-7 using CVAE.

```python
BATCH_SIZE = 64
EPOCHS = 50
LR = 1e-3
LATENT_DIM = 8
# 1 hidden layer
HIDDEN_DIMS = [128]
KLD_WEIGHT = 0.00025

VAE_TEST_SET_LOSS:  11.72
CVAE_TEST_SET_LOSS:  0.2615
```

#figure(
  image("./img/vae_test_sample.png"),
  caption: "Samples generated by VAE on MNIST dataset"

)

#figure(
  image("./img/cvae_test_sample.png"),
  caption: "Samples generated by CVAE on MNIST dataset(digits 0-7, 8 samples each)"

)<cvae_test>



= Analysis of ARC-AGI-2 Dataset

ARC-AGI-2 dataset is a more complex dataset compared to MNIST, and also far more diffcult.

Here is the structure of the ARC-AGI-2 dataset:
- ARC-AGI-2 contains 1,000 public training tasks and 120 public evaluation tasks. Training tasks are stored in `data/training` and evaluation tasks in `data/evaluation`.
- Each task is represented as a JSON file containing a set of input-output pairs (examples) and metadata.
- Each task JSON file contains a dictionary with two fields:
  - `"train"`: demonstration input/output pairs. It is a list of "pairs" (typically 3 pairs).
  - `"test"`: test input/output pairs. It is a list of "pairs" (typically 1-2 pair).
- A "pair" is a dictionary with two fields:
  - `"input"`: the input "grid" for the pair. (A 2D array)
  - `"output"`: the output "grid" for the pair. (A 2D array)



#let json_25d8a9c8 = [
```JSON
{"train": [{
    "input": [
        [2, 9, 2],
        [4, 4, 4],
        [9, 9, 9]],
    "output": [
        [0, 0, 0],
        [5, 5, 5],
        [5, 5, 5]]
},{
    "input": [
        [7, 3, 3],
        [6, 6, 6],
        [3, 7, 7]],
    "output": [
        [0, 0, 0],
        [5, 5, 5],
        [0, 0, 0]]
},{
    "input": [
        [2, 2, 4],
        [2, 2, 4],
        [1, 1, 1]],
    "output": [
        [0, 0, 0],
        [0, 0, 0],
        [5, 5, 5]]
},{
    "input": [
        [4, 4, 4],
        [2, 3, 2],
        [2, 3, 3]],
    "output": [
        [5, 5, 5],
        [0, 0, 0],
        [0, 0, 0]]
}],
"test": [{
    "input": [
        [4, 4, 4],
        [3, 2, 3],
        [8, 8, 8]],
    "output": [
        [5, 5, 5],
        [0, 0, 0],
        [5, 5, 5]]
}]}
```]

A example task from the ARC-AGI-2 dataset is shown below.

#grid(
  columns: (120pt, 1fr),
  json_25d8a9c8, 
  figure(
    image("./img/training_25d8a9c8.png"),
    caption: "Training task 25d8a9c8 from ARC-AGI-2 dataset"
  )
)

Training a VAE or CVAE model on ARC-AGI-2 dataset is more challenging due to the following reasons:
- A task in ARC-AGI-2 is a more complex data compared to MNIST's one single image. Each task contains multiple input-output pairs, and the model needs to learn to generate outputs from inputs condition on the `"train"` examples.
- VAE is a model that learns to reconstruct data from a latent space. specifically in image generation, it is a model that learns to reconstruct images.
- CVAE extends VAE by conditioning the generation process on additional information, such as class labels. However, in ARC-AGI-2 dataset, there are no explicit class labels provided for each task. And is hard to encode the `"train"` examples as class labels.

= Training on ARC-AGI-2 Dataset

== VAE

=== Data Preparation

We apply a 32x32 padding to all grids. For a given task, assuming it contains three training pairs (otherwise skip this data point) and one test pair (otherwise split into multiple data points with identical training pairs but only one test pair). We divide these into input and output:
- Input: Concatenate all input/output pairs from the train pairs (6 x 32x32) with the input grid from the test pair (1 x 32x32) as the input.
- Output: Use the output grid from the test pair (1 x 32x32) as the output.

=== Adjustments to Model and Training

*One-Hot Embedding*:

We apply one-hot encoding at `ArcVAE.encode` to handle the discrete color values in ARC grids. Now in `ArcVAE.encode`, the input is an integer Tensor of shape `(B, 7, 32, 32)`. We use `F.one_hot` to expand it into `(B, 7, 32, 32, 10)`, then reshape it to `(B, 70, 32, 32)`. 


*KLD Annealing*:

The ARC task is challenging. To prevent posterior collapse (where the decoder disregards latent variables), we incorporated simple KL divergence weight annealing into the training loop. This allows the model to first learn reconstruction (with loss primarily derived from cross-entropy), gradually introducing regularisation constraints thereafter.

=== Results
```bash
uv run ./arc_vae.py --epochs 50 --batch-size 32
```

```python
BATCH_SIZE = 32
EPOCHS = 50
LR = 1e-4
LATENT_DIM = 256
# 3 hidden layers
HIDDEN_DIMS = [128, 256, 512]
# KLD_WEIGHT Annealing
KLD_WEIGHT = min(0.01, (epoch / 20) * 0.01)

# The similarity metrics on the evaluation set
VAL_ACC_RATE:  0.7242
# The exact match rate on the evaluation set
VAL_EXACT_MATCH_RATE:  0.0000
```
#figure(
  image("./img/arcvae.png"),
  caption: "Samples generated by VAE on ARC-AGI-2 dataset on evaluation set"
)


== CVAE

=== Data Preparation

Almost the same as VAE, except that we treat all the input-output pairs in the `"train"` examples as conditions to train the CVAE model. The input data is the input grid from the test pair (1 x 32x32), and the output data is the corresponding output grid from the test pair (1 x 32x32).

=== Adjustments to Model and Training

Almost the same as VAE, with one additional adjustment.

*Condition Encoder*:

We add a condition encoder in `ArcCVAE` to encode the condition data into a latent representation. 

=== Results

```bash
uv run ./arc_cvae.py --epochs 50 --batch-size 32
```

```python
BATCH_SIZE = 32
EPOCHS = 50
LR = 1e-4
LATENT_DIM = 256
# 3 hidden layers
HIDDEN_DIMS = [128, 256, 512]
# KLD_WEIGHT Annealing
KLD_WEIGHT = min(0.01, (epoch / 20) * 0.01)

# The similarity metrics on the evaluation set
VAL_ACC_RATE:  0.7444
# The exact match rate on the evaluation set
VAL_EXACT_MATCH_RATE:  0.0000
```

#figure(
  image("./img/arccvae.png"),
  caption: "Samples generated by CVAE on ARC-AGI-2 dataset on evaluation set"
)

= Summary

Above experiments show that both VAE and CVAE can be trained on the ARC-AGI-2 dataset, and achieve reasonable performance in similarity metrics. However, both models still struggle to achieve exact matches on the evaluation set, indicating the complexity of the ARC tasks.

CVAE seems to perform slightly better than VAE in terms of similarity metrics, likely due to its ability to leverage the condition information from the training examples. But we doubt that because the condition encoder is not poIrful enough to capture the complex relationships in the ARC tasks.

= Reflections

In the beginning, we wanted to implement a general-use VAE/CVAE model that can test on different datasets, including MNIST and ARC-AGI-2. But due to the significantly different nature of these datasets, and the complexity of ARC tasks, we had to make specific adjustments to the models and training procedures for ARC-AGI-2. But we still wander if there is a better way to design a more general model that can handle both simple and complex datasets effectively.

In the analysis of ARC-AGI-2 dataset, we know that it is a very challenging dataset for generative models like VAE/CVAE. So we considering a very bad performance on ARC-AGI-2 dataset is expected. But in the end, if we considering the similarity metrics, both VAE and CVAE achieve reasonable performance.

= References

== Model Implementation References

- #link("https://github.com/lillian039/VARC")

== Some Existing Work on the ARC-AGI-2

Although the ARC-AGI dataset is a corpus for Artificial General Intelligence, some current work is also considering it as a computer vision problem, or a combination of both.

=== ARC Is a Vision Problem!

*Link*:
- #link("https://arxiv.org/abs/2511.14761")
- #link("https://github.com/lillian039/VARC")

They formulate ARC within a vision paradigm, framing it as an image-to-image translation problem.

*Result*:

Their verified result using VARC-ViT-18M (no ensembling) on the ARC-2 private test set from the Kaggle competition: \#30, scores: 7.5

